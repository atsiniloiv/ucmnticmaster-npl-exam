{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3ef2c72",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af0fa15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API_KEY = \"sk-proj-***EQsJNM7EjjiVoCGkj7szMq4Jlxx4x_sfQlEA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa8d730a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e89ede70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='¡Hola! ¿Cómo puedo ayudarte hoy?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 8, 'total_tokens': 17, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-BpsxsF5VZEAGrUHCgO6H1a3HkQqGI', 'finish_reason': 'stop', 'logprobs': None}, id='run-87aec746-f517-4b8c-9230-a4aef3e4aa84-0', usage_metadata={'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "llm.invoke(\"hola\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27af45b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='cual es la capitald e FRancia?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Es paris', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, ToolMessage, BaseMessage\n",
    "\n",
    "chat_history = [\n",
    "    HumanMessage(content=\"cual es la capitald e FRancia?\"),\n",
    "    AIMessage(content=\"Es paris\")\n",
    "]\n",
    "chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482019aa",
   "metadata": {},
   "source": [
    "# State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1004b0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Literal\n",
    "from pydantic import BaseModel\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class AgentState(TypedDict):  # BaseModel\n",
    "    messages: Annotated[list[BaseMessage], add_messages]\n",
    "    decision: Literal['ok', 'fin']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0831290d",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "458b664c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4129cd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def send_telegram_message(mensaje: str) -> str:\n",
    "    \"\"\"\n",
    "    Envía un mensaje de texto en un chat de Telegram. Útil si quiero abrir un siniestro.\n",
    "    \n",
    "    Args:\n",
    "        mensaje (str): El contenido del mensaje.\n",
    "    Returns:\n",
    "        str: Mensaje de confirmación.\n",
    "    \"\"\"\n",
    "    \n",
    "    url_getIdChat = 'https://api.telegram.org/bot'+bot_token+'/getUpdates'\n",
    "    response = requests.get(url_getIdChat)\n",
    "\n",
    "    bot_message = mensaje\n",
    "    url_sendMessage = 'https://api.telegram.org/bot'+bot_token+'/sendMessage?chat_id='+bot_chatId+'&parse_mode=Markdown&text='+bot_message\n",
    "\n",
    "    response = requests.get(url_sendMessage)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe0a028",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a75efe0",
   "metadata": {},
   "source": [
    "# Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3d3c4b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "PROMPT_SUPERVISOR = ChatPromptTemplate.from_template(\n",
    "\"\"\"Un CLIENTE está haciendo consultas a una compañía de Seguros.\n",
    "    \n",
    "    Tu objetivo es determinar si el CLIENTE está hablando de temas de seguros de hogar \\\n",
    "    o bien si empieza a hablar de cosas que no tienen que ver con seguros, siniestros de hogar, \\\n",
    "    coberturas de seguros o similar.\n",
    "    \n",
    "    Tienes que clasificar la última de sus consultas: \n",
    "    - Responde de forma escueta \"ok\" si la conversación es apropiada a \\\n",
    "    una conversación de seguros.\n",
    "    - Responde de forma escueta \"fin\" si el CLIENTE consultar cosas que \\\n",
    "    no tienen que ver con seguros.\n",
    "    \n",
    "    Aquí tienes la conversación :\n",
    "    --------------------------- \n",
    "    {messages}\n",
    "    ---------------------------\n",
    "\n",
    "    Tu respuesta (ok/fin) :  \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "PROMPT_ASESOR = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Un CLIENTE que está haciendo consultas a una compañía de Seguros.\n",
    "    \n",
    "    Tu objetivo es proporcionarle una respuesta en función del conocimiento de las coberturas.\n",
    "    \n",
    "    Instrucciones:\n",
    "    - Si la pregunta no tiene que ver con coberturas, respóndele como sepas hacerlo. \\\n",
    "    Pero no te enredes demasiado.\n",
    "    - Si ves que tiene un problema de verdad, le preguntas si quiere abrir un \\\n",
    "    ticket de Siniestro. \n",
    "    - Si quiere abrir un siniestro, envías un Telegram cuando sepas lo que le ha ocurrido con \\\n",
    "    exactitud y sepas también su email. Cuando no sepas alguna de estas cosas, le preguntas.\n",
    "    \n",
    "    En cualquier caso, se amable y muy educado.\n",
    "    \n",
    "    Aquí tienes la conversación :\n",
    "    --------------------------- \n",
    "    {messages}\n",
    "    ---------------------------\n",
    "    \"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43acb16b",
   "metadata": {},
   "source": [
    "# Nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0446134b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervisor_action(state: AgentState):\n",
    "    \n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-4o\",\n",
    "        temperature=0\n",
    "    )\n",
    "    chain = PROMPT_SUPERVISOR | llm\n",
    "    response = chain.invoke({\"messages\": state['messages']})\n",
    "    decision = response.content\n",
    "#     print(f\">>> Supervisor: {decision}\")\n",
    "    return {\n",
    "        \"decision\": decision\n",
    "    }\n",
    "\n",
    "def asesor_action(state: AgentState):\n",
    "    \n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0.5\n",
    "    )\n",
    "    llm = llm.bind_tools([send_telegram_message])\n",
    "    chain = PROMPT_ASESOR | llm\n",
    "    response = chain.invoke({\"messages\": state['messages']})\n",
    "    answer = response.content\n",
    "    print(f\">>> Asesor: {answer}\")\n",
    "    return {\n",
    "        \"messages\": [response]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8920176a",
   "metadata": {},
   "source": [
    "# Routing logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0bdd2729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervisor_decision(state: AgentState):\n",
    "    if state['decision'] == 'ok':\n",
    "        return \"Asesor\"\n",
    "    else:\n",
    "        return END\n",
    "    \n",
    "def asesor_decision(state: AgentState):\n",
    "    messages = state.get(\"messages\", [])\n",
    "    ai_message = messages[-1]\n",
    "    if hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls)>0:\n",
    "        return \"tools\"\n",
    "    else:\n",
    "        return END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939b7c08",
   "metadata": {},
   "source": [
    "# Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "70a102ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, END, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "def update_graph():\n",
    "    \n",
    "    graph = StateGraph(AgentState)\n",
    "    \n",
    "    tools_node = ToolNode([send_telegram_message])\n",
    "    \n",
    "    # nodos\n",
    "    graph.add_node(\"Supervisor\", supervisor_action)\n",
    "    graph.add_node(\"Asesor\", asesor_action)\n",
    "    graph.add_node(\"tools\", tools_node)\n",
    "    \n",
    "    # edges\n",
    "    graph.add_edge(START, \"Supervisor\")\n",
    "    graph.add_conditional_edges(\"Supervisor\", supervisor_decision)\n",
    "#     graph.add_edge(\"Asesor\", END)\n",
    "    graph.add_conditional_edges(\"Asesor\", asesor_decision)\n",
    "    graph.add_edge(\"tools\", \"Asesor\")\n",
    "    \n",
    "    memory = MemorySaver()\n",
    "    return graph.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c084a1",
   "metadata": {},
   "source": [
    "# Simulación de la conversación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6f1833d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pregunta del usuario: ('stop' para parar)# hola se me ha roto el microondas, me lo cubre el seguro?\n",
      ">>> Asesor: Hola, gracias por tu consulta. Lamentablemente, los daños a electrodomésticos como el microondas suelen no estar cubiertos por los seguros de hogar estándar, a menos que se trate de un daño específico cubierto en tu póliza. \n",
      "\n",
      "Si el daño fue causado por un evento cubierto, como un incendio o una inundación, podrías tener derecho a una reclamación. ¿Te gustaría abrir un ticket de siniestro para que podamos revisarlo? Si es así, necesitaría saber más detalles sobre lo ocurrido y tu dirección de correo electrónico.\n",
      "\n",
      "Pregunta del usuario: ('stop' para parar)# si, me gustaría abrir un ticket de siniestro\n",
      ">>> Asesor: ¡Perfecto! Para poder abrir el ticket de siniestro, necesito que me proporciones más detalles sobre lo ocurrido con el microondas. Además, por favor indícame tu dirección de correo electrónico para que podamos contactarte.\n",
      "\n",
      "Pregunta del usuario: ('stop' para parar)# meti una cuchara en el microondas y explot todo. mi correo electroico es efc@ucm.es\n",
      ">>> Asesor: \n",
      "ENTRA\n",
      ">>> Asesor: ¡Hola! He registrado el siniestro relacionado con tu microondas. Se ha enviado un mensaje a través de Telegram con la información que proporcionaste. Si necesitas más ayuda o tienes alguna otra consulta, no dudes en decírmelo. ¡Estoy aquí para ayudarte!\n",
      "\n",
      "Pregunta del usuario: ('stop' para parar)stop\n"
     ]
    }
   ],
   "source": [
    "# hola se me ha roto el microondas, me lo cubre el seguro?\n",
    "# si, me gustaría abrir un ticket de siniestro\n",
    "# meti una cuchara en el microondas y explot todo. mi correo electroico es efc@ucm.es\n",
    "\n",
    "graph = update_graph()\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "user_input = input(\"\\nPregunta del usuario: ('stop' para parar)\")\n",
    "while user_input != 'stop':\n",
    "    messages = graph.invoke(\n",
    "        input={\"messages\": [HumanMessage(content=f\"{user_input}\")]},\n",
    "        config=config\n",
    "    )\n",
    "    user_input = input(\"\\nPregunta del usuario: ('stop' para parar)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c5525d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "# hola se me ha roto el microondas, me lo cubre el seguro?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hola, gracias por tu consulta. Lamentablemente, los daños a electrodomésticos como el microondas suelen no estar cubiertos por los seguros de hogar estándar, a menos que se trate de un daño específico cubierto en tu póliza. \n",
      "\n",
      "Si el daño fue causado por un evento cubierto, como un incendio o una inundación, podrías tener derecho a una reclamación. ¿Te gustaría abrir un ticket de siniestro para que podamos revisarlo? Si es así, necesitaría saber más detalles sobre lo ocurrido y tu dirección de correo electrónico.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "# si, me gustaría abrir un ticket de siniestro\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "¡Perfecto! Para poder abrir el ticket de siniestro, necesito que me proporciones más detalles sobre lo ocurrido con el microondas. Además, por favor indícame tu dirección de correo electrónico para que podamos contactarte.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "# meti una cuchara en el microondas y explot todo. mi correo electroico es efc@ucm.es\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  send_telegram_message (call_jmFTe3DBFDkBCWuFQoDvgHuh)\n",
      " Call ID: call_jmFTe3DBFDkBCWuFQoDvgHuh\n",
      "  Args:\n",
      "    mensaje: Se ha reportado un siniestro: El cliente metió una cuchara en el microondas y explotó. Su correo electrónico es efc@ucm.es.\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: send_telegram_message\n",
      "\n",
      "{\"ok\":true,\"result\":{\"message_id\":265,\"from\":{\"id\":1060305894,\"is_bot\":true,\"first_name\":\"\\ud83c\\udfc1script\",\"username\":\"edustratiobot\"},\"chat\":{\"id\":125821294,\"first_name\":\"Eduardo\",\"last_name\":\"FC\",\"type\":\"private\"},\"date\":1751711199,\"text\":\"Se ha reportado un siniestro: El cliente meti\\u00f3 una cuchara en el microondas y explot\\u00f3. Su correo electr\\u00f3nico es efc@ucm.es.\",\"entities\":[{\"offset\":112,\"length\":10,\"type\":\"email\"}]}}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "¡Hola! He registrado el siniestro relacionado con tu microondas. Se ha enviado un mensaje a través de Telegram con la información que proporcionaste. Si necesitas más ayuda o tienes alguna otra consulta, no dudes en decírmelo. ¡Estoy aquí para ayudarte!\n"
     ]
    }
   ],
   "source": [
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84156503",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c393938",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b54ef81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('rotten_tomatoes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c96a0ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 8530\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c833c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef26d891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'].__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e4ae696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .',\n",
       "  'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\\'s expanded vision of j . r . r . tolkien\\'s middle-earth .',\n",
       "  'effective but too-tepid biopic',\n",
       "  'if you sometimes like to go to the movies to have fun , wasabi is a good place to start .'],\n",
       " 'label': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f7d2863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2cc884a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84214478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> elapsed time: 0.0009989738464355469s\n",
      ">>> elapsed time: 0.012161970138549805s\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "start = time()\n",
    "dataset['train'][0]['text']\n",
    "end = time()\n",
    "print(f\">>> elapsed time: {end-start}s\")\n",
    "\n",
    "start = time()\n",
    "dataset['train']['text'][0]\n",
    "end = time()\n",
    "print(f\">>> elapsed time: {end-start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a918bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50a44f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1996, 2600, 2003, 16036, 2000, 2022, 1996, 7398, 2301, 1005, 1055, 2047, 1000, 16608, 1000, 1998, 2008, 2002, 1005, 1055, 2183, 2000, 2191, 1037, 17624, 2130, 3618, 2084, 7779, 29058, 8625, 13327, 1010, 3744, 1011, 18856, 19513, 3158, 5477, 4168, 2030, 7112, 16562, 2140, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(dataset['train'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f82be458",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████████████████████████████████████████████████| 8530/8530 [00:00<00:00, 14136.91 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████████████| 1066/1066 [00:00<00:00, 14786.26 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████████████| 1066/1066 [00:00<00:00, 17250.52 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 8530\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenization(x):\n",
    "    return tokenizer(x['text'])\n",
    "\n",
    "# dataset = dataset.map(tokenization, batch_size=1000)\n",
    "dataset = dataset.map(tokenization, batched=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb0c61a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['pokemon', 'type'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "ds = Dataset.from_dict({\n",
    "    \"pokemon\": ['picachu', 'tortuga'],\n",
    "    \"type\": ['water', 'grass']\n",
    "})\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d0bb568a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pokemon</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>picachu</td>\n",
       "      <td>water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tortuga</td>\n",
       "      <td>grass</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pokemon   type\n",
       "0  picachu  water\n",
       "1  tortuga  grass"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690a9387",
   "metadata": {},
   "source": [
    "# Fine tuning (ejemplo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6ba65638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_title</th>\n",
       "      <th>product_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>nunca llego el pedido y el vendedor pasa de to...</td>\n",
       "      <td>No llego nunca</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>no sé como es, porque debería haber llegado ay...</td>\n",
       "      <td>Todavía no ha llegado</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>guantes cómodos, no lo niego, pero de mala cal...</td>\n",
       "      <td>Guantes de baja calidad</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>hasta hoy no he visto el producto. el pedido h...</td>\n",
       "      <td>Muy Mala experiencia</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>no puedo valorarla porque, después de casi una...</td>\n",
       "      <td>Paquete perdido?</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stars                                        review_body             review_title product_category\n",
       "0      1  nunca llego el pedido y el vendedor pasa de to...           No llego nunca           sports\n",
       "1      1  no sé como es, porque debería haber llegado ay...    Todavía no ha llegado           sports\n",
       "2      1  guantes cómodos, no lo niego, pero de mala cal...  Guantes de baja calidad           sports\n",
       "3      1  hasta hoy no he visto el producto. el pedido h...     Muy Mala experiencia           sports\n",
       "4      1  no puedo valorarla porque, después de casi una...         Paquete perdido?           sports"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/eduardofc/data/main/amazon_sports.csv\")\n",
    "df['review_body'] = df['review_body'].str.replace(\"[^a-zA-ZñÑáéíóú .,]\", \"\", regex=True)\n",
    "df['review_body'] = df['review_body'].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a1fdbb10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "good_product\n",
       "0    2438\n",
       "1    2512\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df.stars != 2]\n",
    "df = df[df.stars != 3]\n",
    "df = df[df.stars != 4]\n",
    "\n",
    "df['good_product'] = (df.stars > 3).astype(int)\n",
    "\n",
    "df.groupby('good_product').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5e434e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 3960\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 990\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = Dataset.from_pandas(df)\n",
    "ds = ds.remove_columns(['stars', 'review_title', 'product_category', '__index_level_0__'])\n",
    "ds = ds.rename_column(\"good_product\", \"labels\")\n",
    "ds = ds.rename_column(\"review_body\", \"text\")\n",
    "ds = ds.train_test_split(test_size=.2, seed=99)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ef9afe51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_id = 'distilbert-base-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "34489850",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████████████████████████████████████████████████| 3960/3960 [00:00<00:00, 19367.67 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████| 990/990 [00:00<00:00, 20449.24 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 3960\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 990\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenization(x):\n",
    "    return tokenizer(x['text'], truncation=True)\n",
    "\n",
    "tokenized_ds = ds.map(tokenization, batched=True)\n",
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036c6cec",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "afb672c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "05404fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args.weight_decay = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ce6eed2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fa3021f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\efclprd\\AppData\\Local\\Temp\\ipykernel_9244\\3153126403.py:6: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_ds['train'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d3637733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1485' max='1485' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1485/1485 02:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.136300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.073900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2min 40s\n",
      "Wall time: 2min 40s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1485, training_loss=0.08127385662862348, metrics={'train_runtime': 159.8377, 'train_samples_per_second': 74.325, 'train_steps_per_second': 9.291, 'total_flos': 390056464751904.0, 'train_loss': 0.08127385662862348, 'epoch': 3.0})"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a122a9e0",
   "metadata": {},
   "source": [
    "# Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "91d22bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = trainer.predict(tokenized_ds['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2b3a7ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "preds = np.argmax(preds.predictions, 1)\n",
    "preds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "94667f78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9262626262626262"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "accuracy_score(y_true=tokenized_ds['test']['labels'], y_pred=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "30290dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9257375381485249"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_true=tokenized_ds['test']['labels'], y_pred=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8851fb0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
